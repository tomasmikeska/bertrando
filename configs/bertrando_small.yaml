# Hyperparameters
max_epochs: 1
train_batch_size: 128
val_batch_size: ${train_batch_size}
learning_rate: 5e-5

# Dataset
dataset_path: data/train.txt
val_dataset_path: data/val.txt
tokenizer_path: data/tokenizer/tokenizer.json
max_seq_len: 128

# Model
model:
    n_blocks: 8
    n_heads: 16
    embedding_size: 128
    d_model: 1024
    d_ff: 4096
    dropout: 0.1

# Training
val_check_interval: 0.025
gpus: 1
precision: 16
early_stopping_monitor: val_mlm_loss
early_stopping_delta: 0
early_stopping_patience: 20
early_stopping_mode: 'min'

# Checkpointing
checkpoint:
    save_top_k: 3
    monitor: val_mlm_loss
    mode: min

# Logging
logs:
    path: logs/
    use_comet: True
    log_every_n_steps: 250
